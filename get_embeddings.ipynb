{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from eval import getembeddings\n",
    "from eval import gpuutils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(getembeddings)\n",
    "\n",
    "\n",
    "\n",
    "ocr_data_dir = os.path.join(os.path.dirname(os.getcwd()), 'digitalize_handwritten')\n",
    "groundtruth_dir = os.path.join(ocr_data_dir, 'data')\n",
    "ocr_dir = os.path.join(ocr_data_dir, 'OCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = gpuutils.get_gpu_most_memory()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  4029,  2575,  1012,  1996,  3350,  1997,  1996,  8147,  1010,\n",
      "          9530,  5332, 19225,  2000,  1037, 12109,  6602,  1010,  2612,  1997,\n",
      "          1037,  4964,  2338,  1012,  1035,  2579,  2013,  4654,  5403,  1011,\n",
      "         10861,  2099,  8236,  1010,  1035, 12980,  2013,  4518,  5754, 14663,\n",
      "          3111,  1035, 10217,  2007,  3493,  2139, 10609, 22662,  1010,  1998,\n",
      "          1996,  2085, 26958,  3212, 10967,  8525,  1011,  2035,  2075,  3665,\n",
      "          1998, 14445,  8236,  2030,  2139, 10609, 22662,  1024,  1035,  2036,\n",
      "          2007,  2634,  9547,  1010,  2924,  3964,  1010, 13448,  1005,  1055,\n",
      "         20877, 14643, 10253,  3964,  1025,  1998,  2797, 20877, 14643, 10253,\n",
      "          3964,  1998,  8236,  1997,  3863,  1012,  1021,  1012,  1996,  3259,\n",
      "          1010,  2011,  2049,  2946,  1010,  4338,  1010, 14902,  1010,  2024,\n",
      "          4857,  2791,  1010, 11968,  1011,  1011, 14841, 15431,  2135,  7130,\n",
      "          2005,  9141,  1035,  2579,  2013,  2924,  3259,  1012, 10217,  2062,\n",
      "          2030,  2625,  2007, 25375,  3259,  1998,  2007,  1996,  2413, 23911,\n",
      "         11149,  1012, 12980,  2013,  2035,  1996,  2060,  2682,  3855, 20583,\n",
      "          1024,  1035,  1011, 25212,  2595,  3401, 13876,  2013,  2070,  2397,\n",
      "          3314,  1997, 28889,  8236,  1010,  1999,  4847,  1997,  2946,  1012,\n",
      "          1035,  1022,  1012,  4646,  1997,  1996,  5618,  1997,  1996,  5468,\n",
      "          2875,  1996,  7312,  1997,  1996,  2120,  7016,  1012,  1035,  2579,\n",
      "          2013,  1996,  5096,  1997,  1996,  2455,  4171,  1045,  1024,  1041,\n",
      "          1024,  1996,  3863,  1997,  2061,  2116,  8810,  1997,  1996,  3296,\n",
      "          3965,  1997,  2008,  4171,  2005,  1996,  8810,  1997,  4518,  5754,\n",
      "         14663,  3111,  1012, 12980,  2013,  2035,  1996,  2060, 20583,  2682,\n",
      "          3855,  1012,  1035,  2385, 15476,  9807,   102]], device='cuda:1'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:1')}\n"
     ]
    }
   ],
   "source": [
    "groundtruth_fns = glob(os.path.join(groundtruth_dir, 'BenthamDataset', 'GT', 'GT_Extracted', '*.txt'))\n",
    "\n",
    "for gt_fn in groundtruth_fns:\n",
    "    with open(gt_fn) as f:\n",
    "        gt_text = f.read()\n",
    "\n",
    "    save_path = os.path.join(os.getcwd(), 'data', 'groundtruth_dir', 'bentham', os.path.basename(gt_fn).replace('.txt', '.pt'))\n",
    "\n",
    "    if os.path.exists(save_path): continue # dont need to repeat embedding computation\n",
    "    embedding = getembeddings.get_embedding(gt_text, model=model, tokenizer=tokenizer, device=device, max_len=512)\n",
    "    \n",
    "    break\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
