{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cosine simmilarity between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from eval import getembeddings\n",
    "\n",
    "embedding_dir = os.path.join('data', 'embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\embeddings\\*\\bentham\\*_002_080_001.pt\n",
      "['data\\\\embeddings\\\\ground_truth\\\\bentham\\\\GT_002_080_001.pt', 'data\\\\embeddings\\\\ocr\\\\bentham\\\\ocr_002_080_001.pt', 'data\\\\embeddings\\\\test\\\\bentham\\\\test_002_080_001.pt']\n",
      "['data\\\\embeddings\\\\ocr\\\\bentham\\\\ocr_002_080_001.pt', 'data\\\\embeddings\\\\test\\\\bentham\\\\test_002_080_001.pt']\n",
      "ocr\n",
      "Cosine Similarity: 0.6028253436088562\n",
      "test\n",
      "Cosine Similarity: 0.9981483221054077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnicolow\\AppData\\Local\\Temp\\ipykernel_19644\\3531187463.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gt_embedding = torch.load(gt_embedding_fn)\n",
      "C:\\Users\\jnicolow\\AppData\\Local\\Temp\\ipykernel_19644\\3531187463.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  other_embedding = torch.load(embedding_fn)\n",
      "C:\\Users\\jnicolow\\AppData\\Local\\Temp\\ipykernel_19644\\3531187463.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  other_embedding = torch.load(embedding_fn)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gt_embedding_fns = glob(os.path.join(embedding_dir, 'ground_truth', 'bentham', '*.pt'))\n",
    "\n",
    "for gt_embedding_fn in gt_embedding_fns:\n",
    "    gt_embedding = torch.load(gt_embedding_fn)\n",
    "    \n",
    "    # get other embeddings\n",
    "    print(os.path.join(embedding_dir, '*', 'bentham', f\"*{os.path.basename(gt_embedding_fn).replace('GT', '')}\"))\n",
    "    other_embedding_fns = glob(os.path.join(embedding_dir, '*', 'bentham', f\"*{os.path.basename(gt_embedding_fn).replace('GT', '')}\"))\n",
    "    print(other_embedding_fns)\n",
    "    other_embedding_fns = [f for f in other_embedding_fns if \"ground_truth\" not in os.path.dirname(f)] # remove ground truth\n",
    "    print(other_embedding_fns)\n",
    "    for embedding_fn in other_embedding_fns:\n",
    "        other_embedding = torch.load(embedding_fn)\n",
    "        similarity = cosine_similarity(gt_embedding.reshape(1, -1), other_embedding.reshape(1, -1))\n",
    "        print(os.path.basename(os.path.dirname(os.path.dirname(embedding_fn))))\n",
    "        print(f\"Cosine Similarity: {similarity[0][0]}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97447133]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9660716652870178, 0.8941878080368042, 0.8949460983276367]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from eval import getembeddings\n",
    "from eval import gpuutils\n",
    "from eval import cosinesim\n",
    "\n",
    "import importlib\n",
    "importlib.reload(getembeddings)\n",
    "importlib.reload(cosinesim)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = gpuutils.get_gpu_most_memory()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "gt = \"I dont want no food\"\n",
    "pred = \"No I dont want food\"\n",
    "\n",
    "print(cosinesim.get_cosine_sim_bert_single(gt, pred, model, tokenizer, device=device))\n",
    "\n",
    "gts = [['what the '], ['chicken little'], ['vegan queen']]\n",
    "preds = [['what?'], ['little bird'], ['snowflake']]\n",
    "\n",
    "cosinesim.get_cosine_sim_bert(gt_arr = gts, pred_arr = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "1. Other Propaganda Theorists\n",
    "1.1 Harold Lasswell (1902-1978)\n",
    "As Lippmann was writing propaganda, Harold Lasswell was undertaking empirical analyses of propaganda. In fact, much of the propaganda that Lasswell was examining was actually being written by Lippmann himself (Rogers, 1994).\n",
    "\n",
    "Harold Lasswell (1902-1978) was a prominent scholar in the area of propaganda research. He focused on conducting both quantitative and qualitative analyses of propaganda, and discovering the effect of propaganda on the mass audience (Rogers, 1994). Lasswell is credited with creating the mass communication procedure of content analysis (Rogers, 1944). Generally, content analysis can be defined as, \"...the investigation of communication messages by categorizing message content into classifications in order to measure certain variables” (Rogers, 1954).\n",
    "\n",
    "In an essay entitled \"Contents of Communication,” Lasswell (1946) explains that content analysis should take into account the frequency with which certain symbols appear in a message, the direction in which the symbols try to persuade the audience's opinion, and the intensity of the symbols used. By understanding the content of the message, Lasswell (1946) aims to achieve the goal of understanding the “stream of influence that runs from control to content and from content to audience” (p. 74).\n",
    "\n",
    "This method of content analysis is tied strongly to Lasswell's (1953) early definition of communication, which stated, “Who says what in which channel to whom and with what effects\" (p. 84). Content analysis was essentially the “says what” part of this definition, and Lasswell went on to do a lot of work within this area during the remainder of his career.\n",
    "\n",
    "Aside from understanding the content of propaganda, Lasswell was also interested in how propaganda could shape public opinion. This dealt primarily with understanding the effects of the media. Lasswell was particularly interested in examining the effects of the media in creating public opinion within a democratic system. In this way, Lasswell has created a cycle, whereby the public is limited in the information that is presented to them, and also apprehensive to accept it. However, it is still that information that is affecting their decisions within the democratic system, and is being presented to them by the government. This is an interesting way of viewing the power of the media that is somewhat similar to Lippmann’s theories.\n",
    "\n",
    "1.2 Edward Bernays (1891-1995)\n",
    "At approximately the same time that Lippmann and Lasswell were examining public opinion and propaganda, Edward Bernays (1891-1955) was examining public relations, propaganda, and public opinion. Bernays (1925) defines propaganda as, “a consistent, enduring effort to create or shape events to influence the relations of a public to an enterprise, idea, or group.”\n",
    "\n",
    "Bernays states, “We are governed, our minds are molded, our tastes formed, our ideas suggested, largely by men we have never heard of... Vast numbers of human beings must cooperate in this manner if they are to live together as a smoothly functioning society” (p. 37).\n",
    "\n",
    "Bernays believed that propaganda, when understood in its proper sense, was an essential part of a functioning society. He emphasized the importance of understanding the attitudes of various groups in society, gathering information, and using this knowledge to influence public opinion in the intended direction.\n",
    "\n",
    "Both of these theorists represent a step forward for mass communication theory. They moved away from more mechanical presentations of \"sit-down\" propaganda and moved toward a deeper understanding of the complex processes involved in shaping public opinion and behavior.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88099205]\n",
      "[0.9767121]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "data_dir = r'C:\\Users\\jnicolow\\Documents\\courses\\fall2024\\ICS-661\\final project\\sampleDataset'\n",
    "gt_dir = os.path.join(data_dir, 'input_sample_groundtruth')\n",
    "pred_dir = os.path.join(data_dir, 'ocr_output_scalefactor4')\n",
    "# pred_dir = os.path.join(data_dir, 'ocr_output_scalefactor6')\n",
    "\n",
    "gt_fns = glob(os.path.join(gt_dir, '*.txt'))\n",
    "\n",
    "\n",
    "for i, gt_fn in enumerate(gt_fns):\n",
    "    pred_fn = os.path.join(pred_dir, os.path.basename(gt_fn))\n",
    "    if os.path.join(pred_fn):\n",
    "        with open(gt_fn, \"r\") as file:  \n",
    "            gt_text = file.read()\n",
    "        \n",
    "        with open(pred_fn, \"r\") as file:  \n",
    "            pred_text = file.read()\n",
    "\n",
    "        cosine_sim = cosinesim.get_cosine_sim_bert_single(gt_text, pred_text, model, tokenizer, device=device)\n",
    "        print(cosine_sim)\n",
    "        cosine_sim = cosinesim.get_cosine_sim_bert_single(gt_text, test, model, tokenizer, device=device)\n",
    "        print(cosine_sim)\n",
    "\n",
    "        # print(pred_text)\n",
    "\n",
    "        # gt_embedding = getembeddings.get_embedding_pool(gt_text, model, tokenizer=tokenizer, device=device, max_len=512)\n",
    "        # pred_embedding = getembeddings.get_embedding_pool(pred_text, model, tokenizer=tokenizer, device=device, max_len=512)\n",
    "    break\n",
    "    if i == 5: break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'False'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jnicolow\\AppData\\Local\\anaconda3\\envs\\ocr\\lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pipeline for grammar correction\n",
    "model = pipeline(\"text2text-generation\", model=\"t5-base\", device=gpuutils.get_gpu_most_memory())\n",
    "\n",
    "# Correct OCR text\n",
    "bad_text = \"Ths is a badd OCR outputt\"\n",
    "corrected_text = model(f\"fix grammar: {bad_text}\")\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jnicolow\\AppData\\Local\\anaconda3\\envs\\ocr\\lib\\site-packages\\transformers\\generation\\utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (51). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 51, its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T. Fix grammar: Ths is a badd OCR outputt. T. Fix spelling: T-s-t-s. T-S-T-S. T.-S-R-R. Th-S\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\", device=gpuutils.get_gpu_most_memory())\n",
    "\n",
    "input_text = \"Fix grammar: Ths is a badd OCR outputt.\"\n",
    "result = model(input_text, max_new_tokens=50)\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammar and typos: Ths is a badd OCR outputt.\n",
      "\n",
      "I have a problem with the following code:\n",
      "\n",
      "I have a problem with the following code:\n",
      "\n",
      "I have a problem with the following code:\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "input_text = \"Fix grammar and typos: Ths is a badd OCR outputt.\"\n",
    "result = model(input_text, max_length=50)\n",
    "print(result[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
